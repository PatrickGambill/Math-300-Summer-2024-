{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Lecture 5 #\n",
    "\n",
    "## Singular Value Decomposition ##\n",
    "\n",
    "Singular Value Decomposition is way of decomposing any real matrix $A$ into $3$ matrices. Suppose $A$ is a $m \\times n$ matrix. Then $U$ Is a $m \\times m$ orthogonal matrix, $V$ is a $n \\times n$ orthogonal matrix, and $\\Sigma$ is a $m \\times n $ matrix that can only contain non-zero values on its main diagonal. These non-zero values are called the singular values of $A$. These matrices multiply as:\n",
    "\n",
    "$$ A = U \\Sigma V^T $$\n",
    "\n",
    "There is a numpy function to find these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 2.]\n",
      " [3. 4.]\n",
      " [5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1,2],[3,4],[5,6]])\n",
    "\n",
    "U,S,VT = np.linalg.svd(A)\n",
    "\n",
    "#S is not in the right form. Numpy will return a 1D array consisting of the main diagonal values only. We will convert this into sigma\n",
    "sigma = np.zeros((A.shape))\n",
    "\n",
    "#Populate the main diagonal terms of sigma\n",
    "for i in range(len(S)):\n",
    "    sigma[i,i] = S[i]\n",
    "\n",
    "#This is to show the decomposition is correct\n",
    "print(U@sigma@VT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 1 #\n",
    "Take the music opinion data from the board, and put it into a matrix. Find the SVD of this matrix. Make sure you convert sigma to the appropriate dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U is:\n",
      "[[-0.44882861 -0.41387335  0.00988244 -0.38776707 -0.17054265 -0.28437753\n",
      "  -0.50385481  0.13236287 -0.29615238 -0.08803154]\n",
      " [-0.21733966  0.35360705  0.11131334 -0.29549759 -0.34280064  0.35867894\n",
      "   0.27926187 -0.07239106 -0.25725286 -0.57658173]\n",
      " [-0.23614914  0.15212644 -0.09221147  0.36381991 -0.1668863  -0.44287455\n",
      "  -0.08430657 -0.71678387  0.06295289 -0.17715691]\n",
      " [-0.18242084  0.51989896  0.57871846  0.06357639 -0.26103771 -0.12405186\n",
      "  -0.15530496  0.16368806 -0.04688643  0.46992488]\n",
      " [-0.33965405  0.28937765  0.1170837  -0.0846698   0.86196117 -0.02746142\n",
      "  -0.06298409 -0.03854124 -0.11567864 -0.13210973]\n",
      " [-0.41473402 -0.25181657  0.05782449  0.42254076 -0.01935928  0.70130434\n",
      "  -0.21680534 -0.14674428  0.02681184  0.14573902]\n",
      " [-0.43998277 -0.34538361  0.14986373  0.03200243  0.00538917 -0.20996773\n",
      "   0.75910611  0.05725816 -0.01411321  0.19951634]\n",
      " [-0.24228832  0.21164242 -0.37575783  0.51518449 -0.08466258 -0.1714789\n",
      "  -0.01942956  0.62360859 -0.01538902 -0.25262637]\n",
      " [-0.27974933  0.07340942 -0.02796743 -0.29623995 -0.05597776  0.02073062\n",
      "  -0.06739159  0.07235618  0.89704624 -0.09888789]\n",
      " [-0.21060683  0.30595949 -0.68015156 -0.29326438 -0.06106608  0.111575\n",
      "   0.06916021 -0.12745909 -0.14396159  0.50546165]]\n",
      "sigma is:\n",
      "[[27.90807928  0.          0.          0.        ]\n",
      " [ 0.         15.02997755  0.          0.        ]\n",
      " [ 0.          0.          3.44904278  0.        ]\n",
      " [ 0.          0.          0.          3.05663046]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "VT is: \n",
      "[[-0.51215351 -0.36382333 -0.62910633 -0.45777352]\n",
      " [ 0.22319552 -0.73498222  0.51761255 -0.37691132]\n",
      " [-0.09586686 -0.5646494  -0.1777148   0.80024874]\n",
      " [-0.82382771  0.09276189  0.55201442  0.08934886]]\n",
      "The product of these matrices is:\n",
      "[[ 6.  9.  4.  8.]\n",
      " [ 5. -2.  6.  1.]\n",
      " [ 3.  1.  6.  2.]\n",
      " [ 4. -5.  7.  1.]\n",
      " [ 6.  0.  8.  3.]\n",
      " [ 4.  7.  6.  7.]\n",
      " [ 5.  8.  5.  8.]\n",
      " [ 3.  1.  7.  1.]\n",
      " [ 5.  2.  5.  3.]\n",
      " [ 5.  0.  6. -1.]]\n"
     ]
    }
   ],
   "source": [
    "#Here is the data\n",
    "A = np.array([[6,9,4,8],[5,-2,6,1],[3,1,6,2],[4,-5,7,1],[6,0,8,3],[4,7,6,7],[5,8,5,8],[3,1,7,1],[5,2,5,3],[5,0,6,-1]])\n",
    "\n",
    "U,S,VT = np.linalg.svd(A)\n",
    "\n",
    "sigma = np.zeros(A.shape)\n",
    "for i in range(len(S)):\n",
    "    sigma[i,i] = S[i]\n",
    "\n",
    "#Here are the matrices we got\n",
    "print(\"U is:\")\n",
    "print(U)\n",
    "print(\"sigma is:\")\n",
    "print(sigma)\n",
    "print(\"VT is: \")\n",
    "print(VT)\n",
    "\n",
    "#Verify the decomposition\n",
    "print(\"The product of these matrices is:\")\n",
    "print(np.round(U@sigma@VT,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Notice that the first two singular values are much larger than the last two. If we remove the last two singular values, the resulting matrix is still close to the original matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27.90807928  0.          0.          0.        ]\n",
      " [ 0.         15.02997755  0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "[[ 5.0268  9.1292  4.6603  8.0786]\n",
      " [ 4.2927 -1.6994  6.5668  0.7735]\n",
      " [ 3.8857  0.7173  5.3296  2.1552]\n",
      " [ 4.3514 -3.891   7.2475 -0.6147]\n",
      " [ 5.8255  0.252   8.2146  2.7   ]\n",
      " [ 5.0831  6.9928  5.3225  6.725 ]\n",
      " [ 5.1301  8.2828  5.0379  7.5776]\n",
      " [ 4.1731  0.1221  5.9004  1.8964]\n",
      " [ 4.2448  2.0295  5.4827  3.1581]\n",
      " [ 4.0366 -1.2414  6.0779  0.9574]]\n"
     ]
    }
   ],
   "source": [
    "sigma = np.zeros(A.shape)\n",
    "for i in range(2):\n",
    "    sigma[i,i] = S[i]\n",
    "    \n",
    "print(sigma)\n",
    "\n",
    "print(np.round(U@sigma@VT,4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This process of removing small singular values is a popular dimension reduction technique. Once we have performed this process, we often will try to check the similarity between different columns of the dimension reduced $A$. The similarity we will use is the cosine similarity. Recall for vectors $\\vec{a}$, and $\\vec{b}$:\n",
    "\n",
    "$$ \\frac{\\vec{a} \\cdot \\vec{b} }{|\\vec{a}||\\vec{b}|} = \\cos (\\theta)$$\n",
    "\n",
    "The value of $\\cos (\\theta)$ Is called the cosine similarity. The more similar two of the columns of $A$ are, the larger the cosine similarity. We can make the similarity matrix for $A$, by taking row $i$ column $j$ of the similarity matrix the cosine similarity of columns $i$ and $j$ Let's compute this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the similarity matrix of M\n",
      "[[1.         0.49058926 0.9826445  0.79735421]\n",
      " [0.49058926 1.         0.32043289 0.91706797]\n",
      " [0.9826445  0.32043289 1.         0.67156506]\n",
      " [0.79735421 0.91706797 0.67156506 1.        ]]\n",
      "This is the similarity matrix of A\n",
      "[[1.         0.4789934  0.9499805  0.76311591]\n",
      " [0.4789934  1.         0.32206137 0.86731298]\n",
      " [0.9499805  0.32206137 1.         0.65137868]\n",
      " [0.76311591 0.86731298 0.65137868 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#This is the matrix we found with the dimensions reduced\n",
    "M = U@sigma@VT\n",
    "\n",
    "#This is the cosine similarity of the dimension reduced matrix\n",
    "similaritym = np.array([[sum(M[:,i]*M[:,j]/(np.sqrt(np.sum(M[:,i]**2))*np.sqrt(np.sum(M[:,j]**2)))) for i in range(4)] for j in range(4)])\n",
    "print(\"This is the similarity matrix of M:\")\n",
    "print(similaritym)\n",
    "\n",
    "#This is the cosine similarity of the original matrix\n",
    "similaritya = np.array([[sum(A[:,i]*A[:,j]/(np.sqrt(np.sum(A[:,i]**2))*np.sqrt(np.sum(A[:,j]**2)))) for i in range(4)] for j in range(4)])\n",
    "print(\"This is the similarity matrix of A:\")\n",
    "print(similaritya)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Recall songs $1$ and $3$ were both Legend of Zelda piano arrangements, and songs $2$ and $4$ were both European metal songs. We expect each genre of song to dictate the information about similarity. Notice the dimension reduced matrix, $M$ gives higher similarity scores for songs of the same genre, but only slightly different scores for songs of different genres. This suggests dimension reduction will help us with classifying these songs (without the need to store as much information).\n",
    "\n",
    "As a final thought, these similarity matrices can be converted into clusters using algorithms like $K$-means. This algorithm has been implemented in sci-kit-learn. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}